from flask import Flask, request, jsonify, render_template
import pandas as pd
import numpy as np
from keplergl import KeplerGl
import requests
import json
from datetime import datetime
import time
import warnings
import os
import tempfile
import io
warnings.filterwarnings('ignore')

app = Flask(__name__)

class WarehouseFixedVisualizer:
    def __init__(self):
        self.coordinate_cache = {}
        self.zipcode_api_base = "http://api.zippopotam.us/us/"
        self.all_data = None
        self.warehouse_mapping = None

    def analyze_warehouse_ids(self, df_sample):
        """åˆ†ææ–‡ä»¶ä¸­çš„warehouse IDå¹¶æ¨æµ‹åœ°ç†ä½ç½®"""
        print("ğŸ” åˆ†æwarehouse IDå¹¶æ¨æµ‹åœ°ç†ä½ç½®...")
        
        warehouse_counts = df_sample['warehouse_name'].value_counts()
        print(f"ğŸ“Š å‰{len(df_sample)}è¡Œä¸­å‘ç°çš„warehouse:")
        for warehouse, count in warehouse_counts.items():
            print(f"   {warehouse}: {count} æ¬¡")

        # æ ¹æ®warehouse IDæ¨æµ‹åœ°ç†ä½ç½®å’Œé‚®ç¼–
        warehouse_zipcode_mapping = {
            # NJç³»åˆ— - æ–°æ³½è¥¿å· (Newark, Elizabethç­‰ç‰©æµä¸­å¿ƒ)
            'NJ9': '07114',          # Newark, NJ - ä¸»è¦ç‰©æµä¸­å¿ƒ
            'NJ8': '07201',          # Elizabeth, NJ - æ¸¯å£ç‰©æµä¸­å¿ƒ
            'NJ7': '08817',          # Edison, NJ - ä»“å‚¨åŒº
            'NJ-Main': '07306',      # Jersey City, NJ

            # TXç³»åˆ— - å¾·å…‹è¨æ–¯å· (è¾¾æ‹‰æ–¯-æ²ƒæ–¯å ¡åœ°åŒº)
            'TX8828': '75261',       # Dallas, TX - ä¸»è¦ç‰©æµæ¢çº½
            'TX8829': '76155',       # Fort Worth, TX
            'TX-DFW': '75063',       # Irving, TX - DFWæœºåœºé™„è¿‘
            'TX-Houston': '77032',   # Houston, TX - èˆ¹è¿ä¸­å¿ƒ

            # WNTç³»åˆ— - æ¨æµ‹ä¸ºWest Coast + NT (Northwest Terminal)
            'WNT485': '90248',       # Gardena, CA - æ´›æ‰çŸ¶åœ°åŒºç‰©æµä¸­å¿ƒ
            'WNT486': '91761',       # Ontario, CA - å†…é™†å¸å›½ç‰©æµåŒº
            'WNT487': '92408',       # San Bernardino, CA

            # CAç³»åˆ— - åŠ åˆ©ç¦å°¼äºšå·
            'CA-LA': '90058',        # Los Angeles, CA - å·¥ä¸šåŒº
            'CA-SF': '94080',        # South San Francisco, CA
            'CA-OAK': '94621',       # Oakland, CA - æ¸¯å£åŒº

            # ILç³»åˆ— - ä¼Šåˆ©è¯ºä¼Šå· (èŠåŠ å“¥åœ°åŒº)
            'IL-CHI': '60638',       # Chicago, IL - ç‰©æµåŒº
            'IL9': '60106',          # Bensenville, IL - O'Hareé™„è¿‘

            # GAç³»åˆ— - ä½æ²»äºšå· (äºšç‰¹å…°å¤§)
            'GA-ATL': '30349',       # Atlanta, GA - æœºåœºç‰©æµåŒº

            # FLç³»åˆ— - ä½›ç½—é‡Œè¾¾å· (è¿ˆé˜¿å¯†)
            'FL-MIA': '33166',       # Miami, FL - ç‰©æµä¸­å¿ƒ

            # é€šç”¨/æœªçŸ¥ä»“åº“ - é»˜è®¤ä¸ºä¸»è¦ç‰©æµä¸­å¿ƒ
            'Unknown': '07114',      # é»˜è®¤æ–°æ³½è¥¿Newark
            'MAIN': '10001',         # çº½çº¦ä¸»ä»“
            'NYC-Main': '11378',     # Queens, NY - ç‰©æµåŒº
        }

        print(f"\nğŸ“ Warehouseé‚®ç¼–æ˜ å°„è¡¨:")
        for warehouse, zipcode in warehouse_zipcode_mapping.items():
            print(f"   {warehouse} â†’ {zipcode}")

        return warehouse_zipcode_mapping

    def get_warehouse_zipcode(self, warehouse_name):
        """æ ¹æ®warehouseåç§°è·å–å¯¹åº”é‚®ç¼–"""
        if pd.isna(warehouse_name):
            return self.warehouse_mapping['Unknown']

        warehouse_name = str(warehouse_name).strip()

        # ç›´æ¥åŒ¹é…
        if warehouse_name in self.warehouse_mapping:
            return self.warehouse_mapping[warehouse_name]

        # æ¨¡ç³ŠåŒ¹é…
        warehouse_upper = warehouse_name.upper()

        # NJç³»åˆ—åŒ¹é…
        if warehouse_upper.startswith('NJ'):
            return self.warehouse_mapping.get('NJ9', '07114')

        # TXç³»åˆ—åŒ¹é…
        elif warehouse_upper.startswith('TX'):
            return self.warehouse_mapping.get('TX8828', '75261')

        # WNTç³»åˆ—åŒ¹é…
        elif warehouse_upper.startswith('WNT'):
            return self.warehouse_mapping.get('WNT485', '90248')

        # CAç³»åˆ—åŒ¹é…
        elif warehouse_upper.startswith('CA'):
            return self.warehouse_mapping.get('CA-LA', '90058')

        # ILç³»åˆ—åŒ¹é…
        elif warehouse_upper.startswith('IL'):
            return self.warehouse_mapping.get('IL-CHI', '60638')

        # åŒ…å«å…³é”®è¯åŒ¹é…
        elif 'NYC' in warehouse_upper or 'NEW YORK' in warehouse_upper:
            return self.warehouse_mapping.get('NYC-Main', '11378')
        elif 'DALLAS' in warehouse_upper or 'DFW' in warehouse_upper:
            return self.warehouse_mapping.get('TX-DFW', '75063')
        elif 'LA' in warehouse_upper or 'LOS ANGELES' in warehouse_upper:
            return self.warehouse_mapping.get('CA-LA', '90058')
        elif 'CHICAGO' in warehouse_upper:
            return self.warehouse_mapping.get('IL-CHI', '60638')
        elif 'ATLANTA' in warehouse_upper:
            return self.warehouse_mapping.get('GA-ATL', '30349')
        elif 'MIAMI' in warehouse_upper:
            return self.warehouse_mapping.get('FL-MIA', '33166')

        # é»˜è®¤è¿”å›
        return self.warehouse_mapping['Unknown']

    def extract_zipcode(self, zipcode_str):
        """æå–5ä½æ•°é‚®ç¼–"""
        if pd.isna(zipcode_str):
            return None
        zipcode_str = str(zipcode_str).strip()
        zipcode = ''.join(filter(str.isdigit, zipcode_str))[:5]
        return zipcode if len(zipcode) == 5 else None

    def get_coordinates(self, zipcode):
        """è·å–é‚®ç¼–åæ ‡"""
        if not zipcode or zipcode in self.coordinate_cache:
            return self.coordinate_cache.get(zipcode, (None, None))

        try:
            url = f"{self.zipcode_api_base}{zipcode}"
            response = requests.get(url, timeout=5)

            if response.status_code == 200:
                data = response.json()
                if 'places' in data and len(data['places']) > 0:
                    place = data['places'][0]
                    lat, lng = float(place['latitude']), float(place['longitude'])
                    city = place['place name']
                    state = place['state abbreviation']
                    self.coordinate_cache[zipcode] = (lat, lng)
                    print(f"   âœ“ {zipcode}: {city}, {state}")
                    return (lat, lng)

            self.coordinate_cache[zipcode] = (None, None)
            return (None, None)

        except Exception as e:
            print(f"   âœ— {zipcode}: APIè¯·æ±‚å¤±è´¥")
            self.coordinate_cache[zipcode] = (None, None)
            return (None, None)

    def process_timestamp(self, timestamp_str):
        """å¤„ç†æ—¶é—´æˆ³ä¸ºæ ‡å‡†æ ¼å¼"""
        if pd.isna(timestamp_str):
            return None, None

        try:
            if isinstance(timestamp_str, str) and '/' in timestamp_str:
                dt = datetime.strptime(timestamp_str, '%m/%d/%y %H:%M')
                return dt.strftime('%Y-%m-%d'), dt.strftime('%Y-%m-%d %H:%M:%S')
            elif isinstance(timestamp_str, (int, float)):
                if timestamp_str > 1e10:
                    timestamp_str = timestamp_str / 1000
                dt = datetime.fromtimestamp(timestamp_str)
                return dt.strftime('%Y-%m-%d'), dt.strftime('%Y-%m-%d %H:%M:%S')
        except:
            pass
        return None, None

    def process_data(self, df, sample_size=500):
        """å¤„ç†æ‰€æœ‰æ•°æ®ï¼Œä¿®å¤warehouseé‚®ç¼– - å®Œæ•´Colabç‰ˆæœ¬é€»è¾‘"""
        print(f"ğŸ”„ å¼€å§‹å¤„ç†æ•°æ®å¹¶ä¿®å¤warehouseé‚®ç¼– (æ ·æœ¬å¤§å°: {sample_size})...")

        # 1. åˆ†æå¹¶åˆ›å»ºwarehouseæ˜ å°„
        df_sample = df.head(200)  # å…ˆå–200è¡Œåˆ†æwarehouse
        self.warehouse_mapping = self.analyze_warehouse_ids(df_sample)

        # 2. è¯»å–æ•°æ®
        df = df.head(sample_size)
        print(f"\nğŸ“‚ åŸå§‹æ•°æ®: {len(df)} è¡Œ")

        # 3. ä¿®å¤warehouseé‚®ç¼–
        print("ğŸ”§ ä¿®å¤warehouseé‚®ç¼–...")
        df['fixed_warehouse_zipcode'] = df['warehouse_name'].apply(self.get_warehouse_zipcode)

        # æ˜¾ç¤ºä¿®å¤ç»“æœ
        warehouse_fix_stats = df.groupby(['warehouse_name', 'fixed_warehouse_zipcode']).size().reset_index(name='count')
        print("ğŸ“‹ Warehouseé‚®ç¼–ä¿®å¤ç»“æœ:")
        for _, row in warehouse_fix_stats.iterrows():
            print(f"   {row['warehouse_name']} â†’ {row['fixed_warehouse_zipcode']} ({row['count']} æ¡è®°å½•)")

        # 4. å¤„ç†æ—¶é—´æˆ³
        print("\nâ° å¤„ç†æ—¶é—´æˆ³...")
        timestamp_results = df['created_time'].apply(self.process_timestamp)
        df['shipment_date'] = [r[0] for r in timestamp_results]
        df['shipment_datetime'] = [r[1] for r in timestamp_results]

        # 5. æ¸…æ´—ç›®çš„åœ°é‚®ç¼–
        print("ğŸ“® æ¸…æ´—ç›®çš„åœ°é‚®ç¼–...")
        df['destination_zipcode'] = df['shipto_postal_code'].apply(self.extract_zipcode)

        # 6. è¿‡æ»¤æœ‰æ•ˆæ•°æ®
        valid_df = df[
            (df['shipment_date'].notna()) &
            (df['fixed_warehouse_zipcode'].notna()) &
            (df['destination_zipcode'].notna())
        ].copy()

        print(f"âœ… æœ‰æ•ˆæ•°æ®: {len(valid_df)} è¡Œ")

        if len(valid_df) == 0:
            print("âŒ æ²¡æœ‰æœ‰æ•ˆæ•°æ®!")
            return None

        # æ˜¾ç¤ºæ—¥æœŸåˆ†å¸ƒ
        date_counts = valid_df['shipment_date'].value_counts().sort_index()
        print(f"ğŸ“… æ—¥æœŸåˆ†å¸ƒ ({len(date_counts)} å¤©):")
        for date, count in date_counts.items():
            print(f"   {date}: {count} ç¬”")

        # 7. è·å–æ‰€æœ‰é‚®ç¼–çš„åæ ‡
        print(f"\nğŸŒ è·å–é‚®ç¼–åæ ‡...")
        all_zipcodes = list(set(
            valid_df['fixed_warehouse_zipcode'].tolist() +
            valid_df['destination_zipcode'].tolist()
        ))

        print(f"éœ€è¦å¤„ç† {len(all_zipcodes)} ä¸ªå”¯ä¸€é‚®ç¼–")

        successful_coords = 0
        for i, zipcode in enumerate(all_zipcodes):
            coord = self.get_coordinates(zipcode)
            if coord != (None, None):
                successful_coords += 1

            if (i + 1) % 10 == 0:
                print(f"   è¿›åº¦: {i + 1}/{len(all_zipcodes)} | æˆåŠŸ: {successful_coords}")
            time.sleep(0.1)

        success_rate = successful_coords / len(all_zipcodes) * 100
        print(f"âœ… åæ ‡è·å–æˆåŠŸç‡: {successful_coords}/{len(all_zipcodes)} ({success_rate:.1f}%)")

        # 8. æ·»åŠ åæ ‡
        print("ğŸ“ æ·»åŠ åæ ‡ä¿¡æ¯...")

        def get_lat(zipcode):
            coord = self.coordinate_cache.get(zipcode, (None, None))
            return coord[0] if coord and len(coord) == 2 else None

        def get_lng(zipcode):
            coord = self.coordinate_cache.get(zipcode, (None, None))
            return coord[1] if coord and len(coord) == 2 else None

        valid_df['warehouse_lat'] = valid_df['fixed_warehouse_zipcode'].apply(get_lat)
        valid_df['warehouse_lng'] = valid_df['fixed_warehouse_zipcode'].apply(get_lng)
        valid_df['destination_lat'] = valid_df['destination_zipcode'].apply(get_lat)
        valid_df['destination_lng'] = valid_df['destination_zipcode'].apply(get_lng)

        # 9. æœ€ç»ˆè¿‡æ»¤ï¼ˆå¿…é¡»æœ‰åæ ‡ï¼‰
        final_df = valid_df[
            (valid_df['warehouse_lat'].notna()) &
            (valid_df['destination_lat'].notna())
        ].copy()

        print(f"ğŸ¯ æœ€ç»ˆæ•°æ®ï¼ˆå«åæ ‡ï¼‰: {len(final_df)} è¡Œ")

        if len(final_df) == 0:
            print("âŒ æ²¡æœ‰æ•°æ®åŒ…å«æœ‰æ•ˆåæ ‡!")
            return None

        # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
        print(f"\nğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
        final_warehouse_stats = final_df.groupby(['warehouse_name', 'fixed_warehouse_zipcode']).size().reset_index(name='count')
        print("ä»“åº“åˆ†å¸ƒ:")
        for _, row in final_warehouse_stats.iterrows():
            warehouse_coord = self.coordinate_cache.get(row['fixed_warehouse_zipcode'], (None, None))
            if warehouse_coord != (None, None):
                print(f"   {row['warehouse_name']} ({row['fixed_warehouse_zipcode']}): {row['count']} ç¬” â†’ åæ ‡: {warehouse_coord}")

        final_date_counts = final_df['shipment_date'].value_counts().sort_index()
        print("æ—¥æœŸåˆ†å¸ƒ:")
        for date, count in final_date_counts.items():
            print(f"   {date}: {count} ç¬”")

        # 10. åˆ›å»ºKepleræ•°æ®é›†
        print(f"\nğŸ“‹ åˆ›å»ºKepler.glæ•°æ®é›†...")

        kepler_data = pd.DataFrame({
            # åŸºæœ¬ä¿¡æ¯
            'shipment_id': final_df['id'],
            'shipment_date': final_df['shipment_date'],
            'shipment_datetime': final_df['shipment_datetime'],
            'warehouse': final_df['warehouse_name'].fillna('Unknown'),
            'warehouse_zipcode': final_df['fixed_warehouse_zipcode'],

            # åæ ‡ä¿¡æ¯
            'origin_lat': final_df['warehouse_lat'],
            'origin_lng': final_df['warehouse_lng'],
            'dest_lat': final_df['destination_lat'],
            'dest_lng': final_df['destination_lng'],

            # åœ°å€ä¿¡æ¯
            'dest_zipcode': final_df['destination_zipcode'],
            'dest_city': final_df['shipto_city'].fillna('Unknown'),
            'dest_country': final_df['shipto_country_code'].fillna('US'),

            # ä¸šåŠ¡ä¿¡æ¯
            'carrier': final_df['carrier'].fillna('Unknown'),
            'business_type': final_df.get('biz_type', pd.Series(['Standard'] * len(final_df))).fillna('Standard'),
            'weight_kg': final_df.get('gw', pd.Series([1] * len(final_df))).fillna(1),
            'volume_m3': final_df.get('vol', pd.Series([0.1] * len(final_df))).fillna(0.1),
            'packages': final_df.get('pkg_num', pd.Series([1] * len(final_df))).fillna(1)
        })

        # æ·»åŠ è®¡ç®—å­—æ®µ
        kepler_data['distance_km'] = np.sqrt(
            (kepler_data['dest_lat'] - kepler_data['origin_lat'])**2 +
            (kepler_data['dest_lng'] - kepler_data['origin_lng'])**2
        ) * 111

        self.all_data = kepler_data

        print(f"âœ… Kepleræ•°æ®é›†åˆ›å»ºå®Œæˆ: {len(kepler_data)} è¡Œ")
        print(f"ğŸ“… åŒ…å«æ—¥æœŸ: {kepler_data['shipment_date'].nunique()} å¤©")
        print(f"ğŸ¢ åŒ…å«ä»“åº“: {kepler_data['warehouse'].nunique()} ä¸ª")
        print(f"ğŸ“ åŒ…å«ç›®çš„åœ°: {kepler_data['dest_city'].nunique()} ä¸ª")

        return kepler_data

        return kepler_data

    def create_kepler_config_with_filters(self):
        """åˆ›å»ºåŒ…å«è¿‡æ»¤å™¨çš„Kepleré…ç½® - å®Œæ•´Colabç‰ˆæœ¬"""
        return {
            'version': 'v1',
            'config': {
                'mapState': {
                    'latitude': 39.8,
                    'longitude': -95.0,
                    'zoom': 4,
                    'pitch': 0,
                    'bearing': 0
                },
                'visState': {
                    'filters': [
                        {
                            'dataId': ['shipments'],
                            'id': 'date_filter',
                            'name': ['shipment_date'],
                            'type': 'timeRange',
                            'value': [],
                            'enlarged': True,
                            'plotType': 'histogram',
                            'animationWindow': 'free'
                        }
                    ],
                    'layers': [
                        {
                            'id': 'arc_layer',
                            'type': 'arc',
                            'config': {
                                'dataId': 'shipments',
                                'label': 'Shipping Routes',
                                'color': [255, 0, 0],
                                'highlightColor': [252, 242, 26, 255],
                                'columns': {
                                    'lat0': 'origin_lat',
                                    'lng0': 'origin_lng',
                                    'lat1': 'dest_lat',
                                    'lng1': 'dest_lng'
                                },
                                'isVisible': True,
                                'visConfig': {
                                    'opacity': 0.8,
                                    'thickness': 2,
                                    'colorRange': {
                                        'name': 'Global Warming',
                                        'type': 'sequential',
                                        'category': 'Uber',
                                        'colors': ['#5A1846', '#900C3F', '#C70039', '#E3611C', '#F1920E', '#FFC300']
                                    },
                                    'sizeRange': [1, 8],
                                    'targetColor': [255, 0, 0]
                                }
                            }
                        },
                        {
                            'id': 'warehouse_points',
                            'type': 'point',
                            'config': {
                                'dataId': 'shipments',
                                'label': 'Warehouses',
                                'color': [0, 255, 0],
                                'columns': {
                                    'lat': 'origin_lat',
                                    'lng': 'origin_lng'
                                },
                                'isVisible': True,
                                'visConfig': {
                                    'opacity': 0.9,
                                    'radius': 15,
                                    'radiusRange': [10, 30]
                                }
                            }
                        },
                        {
                            'id': 'destination_points',
                            'type': 'point',
                            'config': {
                                'dataId': 'shipments',
                                'label': 'Destinations',
                                'color': [0, 100, 255],
                                'columns': {
                                    'lat': 'dest_lat',
                                    'lng': 'dest_lng'
                                },
                                'isVisible': True,
                                'visConfig': {
                                    'opacity': 0.8,
                                    'radius': 8,
                                    'radiusRange': [5, 20]
                                }
                            }
                        }
                    ]
                }
            }
        }

    def create_kepler_map(self):
        """åˆ›å»ºåŒ…å«æ‰€æœ‰æ•°æ®çš„Kepler.glåœ°å›¾ - å®Œæ•´Colabç‰ˆæœ¬"""
        if self.all_data is None:
            return None

        print(f"ğŸ—ºï¸ åˆ›å»ºåŒ…å«æ‰€æœ‰æ•°æ®çš„Kepler.glåœ°å›¾...")
        print(f"ğŸ“Š æ•°æ®æ€»é‡: {len(self.all_data)} æ¡è¿è¾“è®°å½•")

        # æ˜¾ç¤ºwarehouseåˆ†å¸ƒ
        warehouse_stats = self.all_data.groupby(['warehouse', 'warehouse_zipcode']).size().reset_index(name='count')
        print(f"\nğŸ¢ ä»“åº“åˆ†å¸ƒ:")
        for _, row in warehouse_stats.iterrows():
            print(f"   {row['warehouse']} ({row['warehouse_zipcode']}): {row['count']} ç¬”")

        # åˆ›å»ºåœ°å›¾
        config = self.create_kepler_config_with_filters()
        map_instance = KeplerGl(height=700, width=1200, config=config)
        map_instance.add_data(data=self.all_data, name='shipments')

        print(f"\nâœ… åœ°å›¾åˆ›å»ºå®Œæˆ!")
        print(f"ğŸ›ï¸ ä½¿ç”¨æ–¹æ³•:")
        print(f"   1. åœ°å›¾æ˜¾ç¤ºæ‰€æœ‰ä»“åº“åˆ°ç›®çš„åœ°çš„è¿è¾“è·¯çº¿")
        print(f"   2. ç»¿è‰²åœ†ç‚¹ = ä»“åº“ä½ç½®ï¼ˆåŸºäºä¿®å¤çš„é‚®ç¼–ï¼‰")
        print(f"   3. è“è‰²åœ†ç‚¹ = ç›®çš„åœ°ä½ç½®")
        print(f"   4. çº¢è‰²å¼§çº¿ = è¿è¾“è·¯çº¿")
        print(f"   5. ç‚¹å‡» 'Filters' æ·»åŠ æ—¥æœŸã€ä»“åº“ã€æ‰¿è¿å•†ç­‰è¿‡æ»¤å™¨")

        return map_instance

# å…¨å±€å¯è§†åŒ–å™¨å®ä¾‹
visualizer = WarehouseFixedVisualizer()

@app.route('/')
def index():
    return render_template('index.html')

@app.route('/api/process-data', methods=['POST'])
def process_data():
    """å¤„ç†å‰ç«¯å‘é€çš„JSONæ•°æ®"""
    try:
        # è·å–JSONæ•°æ®
        data = request.get_json()
        
        if not data:
            return jsonify({'error': 'No data received'}), 400
        
        filename = data.get('filename', 'unknown.csv')
        headers = data.get('headers', [])
        csv_data = data.get('data', [])
        
        print(f"ğŸ“‚ æ¥æ”¶åˆ°æ•°æ®å¤„ç†è¯·æ±‚: {filename}")
        print(f"ğŸ“Š æ•°æ®: {len(csv_data)} è¡Œ, {len(headers)} åˆ—")
        print(f"ğŸ“‹ åˆ—å: {headers}")
        
        if not csv_data:
            return jsonify({'error': 'No data to process'}), 400
        
        # å°†JSONæ•°æ®è½¬æ¢ä¸ºDataFrame - æ·»åŠ æ•°æ®æ¸…æ´—
        try:
            # æ¸…æ´—æ•°æ®ï¼Œç¡®ä¿æ‰€æœ‰å€¼éƒ½æ˜¯å¯åºåˆ—åŒ–çš„
            cleaned_data = []
            for row in csv_data:
                cleaned_row = {}
                for key, value in row.items():
                    # å¤„ç†å„ç§æ•°æ®ç±»å‹
                    if isinstance(value, bytes):
                        cleaned_row[key] = value.decode('utf-8', errors='ignore')
                    elif value is None:
                        cleaned_row[key] = ''
                    elif isinstance(value, (int, float, str, bool)):
                        cleaned_row[key] = value
                    else:
                        # å°†å…¶ä»–ç±»å‹è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                        cleaned_row[key] = str(value)
                cleaned_data.append(cleaned_row)
            
            df = pd.DataFrame(cleaned_data)
            print(f"âœ… DataFrameåˆ›å»ºæˆåŠŸ: {len(df)} è¡Œ")
            
            # æ£€æŸ¥å¿…è¦çš„åˆ—
            required_columns = ['warehouse_name', 'created_time', 'shipto_postal_code']
            missing_columns = [col for col in required_columns if col not in df.columns]
            
            if missing_columns:
                return jsonify({
                    'error': f'Missing required columns: {missing_columns}. Available columns: {list(df.columns)}'
                }), 400
            
        except Exception as e:
            print(f"âŒ DataFrameåˆ›å»ºå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return jsonify({'error': f'Failed to create DataFrame: {str(e)}'}), 400
        
        # å¤„ç†æ•°æ®
        try:
            processed_data = visualizer.process_data(df, sample_size=100)
        except Exception as e:
            print(f"âŒ æ•°æ®å¤„ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return jsonify({'error': f'Data processing failed: {str(e)}'}), 500
        
        if processed_data is None:
            return jsonify({'error': 'No valid data found after processing. Please check your CSV format.'}), 400
        
        # åˆ›å»ºKepleråœ°å›¾
        try:
            print("ğŸ—ºï¸ åˆ›å»ºKepler.glåœ°å›¾...")
            map_instance = visualizer.create_kepler_map()
            
            if map_instance is None:
                return jsonify({'error': 'Failed to create map visualization'}), 500
            
            # è·å–HTML
            map_html = map_instance._repr_html_()
            print("âœ… åœ°å›¾HTMLç”ŸæˆæˆåŠŸ")
            
        except Exception as e:
            print(f"âŒ åœ°å›¾åˆ›å»ºå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return jsonify({'error': f'Map creation failed: {str(e)}'}), 500
        
        # ç»Ÿè®¡ä¿¡æ¯ - ç¡®ä¿æ‰€æœ‰å€¼éƒ½æ˜¯å¯åºåˆ—åŒ–çš„
        try:
            stats = {
                'total_records': int(len(processed_data)),
                'unique_warehouses': int(processed_data['warehouse'].nunique()),
                'unique_destinations': int(processed_data['dest_city'].nunique()),
                'date_range': f"{str(processed_data['shipment_date'].min())} â†’ {str(processed_data['shipment_date'].max())}"
            }
            
            print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯: {stats}")
            
        except Exception as e:
            print(f"âŒ ç»Ÿè®¡ä¿¡æ¯ç”Ÿæˆå¤±è´¥: {e}")
            stats = {
                'total_records': len(processed_data),
                'unique_warehouses': 0,
                'unique_destinations': 0,
                'date_range': 'Unknown'
            }
        
        return jsonify({
            'html': map_html,
            'stats': stats,
            'message': 'Data processed successfully using frontend CSV parsing + backend visualization'
        })
        
    except Exception as e:
        print(f"âŒ æ•°æ®å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({'error': f'Processing failed: {str(e)}'}), 500

@app.route('/api/upload', methods=['POST'])
def upload_file():
    """ä¿ç•™åŸæœ‰çš„æ–‡ä»¶ä¸Šä¼ åŠŸèƒ½ä½œä¸ºå¤‡ç”¨"""
    try:
        if 'file' not in request.files:
            return jsonify({'error': 'No file uploaded'}), 400
        
        file = request.files['file']
        if file.filename == '':
            return jsonify({'error': 'No file selected'}), 400
        
        print("ğŸ“‚ æ¥æ”¶åˆ°æ–‡ä»¶ä¸Šä¼ è¯·æ±‚ï¼ˆå¤‡ç”¨æ–¹å¼ï¼‰")
        
        # ä¿å­˜ä¸´æ—¶æ–‡ä»¶
        with tempfile.NamedTemporaryFile(mode='w+b', suffix='.csv', delete=False) as tmp_file:
            file.save(tmp_file.name)
            print(f"ğŸ“ æ–‡ä»¶å·²ä¿å­˜åˆ°ä¸´æ—¶ä½ç½®: {tmp_file.name}")
            
            # è¯»å–CSV
            try:
                df = pd.read_csv(tmp_file.name)
                print(f"ğŸ“Š æˆåŠŸè¯»å–CSV: {len(df)} è¡Œ, {len(df.columns)} åˆ—")
                print(f"ğŸ“‹ åˆ—å: {list(df.columns)}")
            except Exception as e:
                return jsonify({'error': f'Failed to read CSV file: {str(e)}'}), 400
            
            # å¤„ç†æ•°æ®
            try:
                processed_data = visualizer.process_data(df, sample_size=200)
            except Exception as e:
                print(f"âŒ æ•°æ®å¤„ç†å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                return jsonify({'error': f'Data processing failed: {str(e)}'}), 500
            
            if processed_data is None:
                return jsonify({'error': 'No valid data found after processing. Please check your CSV format.'}), 400
            
            # åˆ›å»ºKepleråœ°å›¾
            try:
                print("ğŸ—ºï¸ åˆ›å»ºKepler.glåœ°å›¾...")
                map_instance = visualizer.create_kepler_map()
                
                if map_instance is None:
                    return jsonify({'error': 'Failed to create map visualization'}), 500
                
                # è·å–HTML
                map_html = map_instance._repr_html_()
                print("âœ… åœ°å›¾HTMLç”ŸæˆæˆåŠŸ")
                
            except Exception as e:
                print(f"âŒ åœ°å›¾åˆ›å»ºå¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                return jsonify({'error': f'Map creation failed: {str(e)}'}), 500
            
            # ç»Ÿè®¡ä¿¡æ¯
            stats = {
                'total_records': len(processed_data),
                'unique_warehouses': processed_data['warehouse'].nunique(),
                'unique_destinations': processed_data['dest_city'].nunique(),
                'date_range': f"{processed_data['shipment_date'].min()} â†’ {processed_data['shipment_date'].max()}"
            }
            
            print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯: {stats}")
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            try:
                os.unlink(tmp_file.name)
                print("ğŸ—‘ï¸ ä¸´æ—¶æ–‡ä»¶å·²æ¸…ç†")
            except:
                pass
            
            return jsonify({
                'html': map_html,
                'stats': stats,
                'message': 'Warehouse locations automatically fixed based on ID patterns'
            })
            
    except Exception as e:
        print(f"âŒ ä¸Šä¼ å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({'error': f'Upload processing failed: {str(e)}'}), 500

@app.route('/api/sample')
def download_sample():
    """ç”Ÿæˆå¹¶ä¸‹è½½æ ·æœ¬CSVæ–‡ä»¶"""
    try:
        # åˆ›å»ºæ ·æœ¬æ•°æ® - åŸºäºçœŸå®çš„express_parcelæ ¼å¼
        sample_data = {
            'id': range(1, 21),
            'warehouse_name': [
                'NJ9', 'TX8828', 'CA-LA', 'NJ8', 'WNT485',
                'IL-CHI', 'TX-DFW', 'CA-SF', 'NJ7', 'WNT486',
                'GA-ATL', 'FL-MIA', 'CA-OAK', 'TX8829', 'IL9',
                'NYC-Main', 'NJ-Main', 'WNT487', 'TX-Houston', 'CA-LA'
            ],
            'created_time': [
                '1/15/24 10:30', '1/15/24 11:45', '1/16/24 09:15', '1/16/24 14:20', '1/17/24 16:10',
                '1/17/24 08:45', '1/18/24 13:20', '1/18/24 15:30', '1/19/24 11:15', '1/19/24 17:45',
                '1/20/24 09:30', '1/20/24 12:15', '1/21/24 10:45', '1/21/24 16:30', '1/22/24 14:15',
                '1/22/24 11:00', '1/23/24 13:45', '1/23/24 15:15', '1/24/24 09:45', '1/24/24 17:30'
            ],
            'shipto_postal_code': [
                '10001', '90210', '60601', '33101', '98101',
                '30309', '02101', '19102', '80202', '85001',
                '89101', '37201', '28201', '23219', '46201',
                '55401', '53201', '40201', '70112', '84101'
            ],
            'shipto_city': [
                'New York', 'Beverly Hills', 'Chicago', 'Miami', 'Seattle',
                'Atlanta', 'Boston', 'Philadelphia', 'Denver', 'Phoenix',
                'Las Vegas', 'Nashville', 'Charlotte', 'Richmond', 'Indianapolis',
                'Minneapolis', 'Milwaukee', 'Louisville', 'New Orleans', 'Salt Lake City'
            ],
            'shipto_country_code': ['US'] * 20,
            'carrier': [
                'FedEx', 'UPS', 'DHL', 'USPS', 'Amazon',
                'FedEx', 'UPS', 'DHL', 'USPS', 'Amazon',
                'FedEx', 'UPS', 'DHL', 'USPS', 'Amazon',
                'FedEx', 'UPS', 'DHL', 'USPS', 'Amazon'
            ],
            'biz_type': [
                'Express', 'Standard', 'Economy', 'Express', 'Standard',
                'Economy', 'Express', 'Standard', 'Economy', 'Express',
                'Standard', 'Economy', 'Express', 'Standard', 'Economy',
                'Express', 'Standard', 'Economy', 'Express', 'Standard'
            ],
            'gw': [1.5, 2.3, 0.8, 3.2, 1.1, 2.8, 1.9, 0.5, 4.1, 2.6, 
                   1.8, 3.5, 0.9, 2.4, 1.7, 3.8, 2.1, 1.3, 4.5, 2.9],
            'vol': [0.05, 0.12, 0.03, 0.18, 0.06, 0.15, 0.08, 0.02, 0.25, 0.14,
                    0.09, 0.22, 0.04, 0.13, 0.07, 0.28, 0.11, 0.05, 0.31, 0.16],
            'pkg_num': [1, 1, 2, 1, 3, 1, 2, 1, 1, 2, 
                       1, 1, 3, 1, 2, 1, 1, 2, 1, 3]
        }
        
        sample_df = pd.DataFrame(sample_data)
        
        # åˆ›å»ºå“åº”
        response = app.response_class(
            csv_content,
            mimetype='text/csv',
            headers={'Content-Disposition': 'attachment; filename=sample_express_parcel.csv'}
        )
        
        print("ğŸ“„ æ ·æœ¬CSVæ–‡ä»¶ç”ŸæˆæˆåŠŸ")
        return response
        
    except Exception as e:
        print(f"âŒ æ ·æœ¬æ–‡ä»¶ç”Ÿæˆå¤±è´¥: {e}")
        return jsonify({'error': f'Failed to generate sample file: {str(e)}'}), 500

@app.route('/health')
def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    return jsonify({
        'status': 'healthy',
        'service': 'Express Parcel Visualization',
        'version': '2.0.0',
        'architecture': 'frontend_csv_parsing + backend_visualization',
        'features': [
            'frontend_csv_parsing', 
            'warehouse_location_fix', 
            'kepler_visualization', 
            'zipcode_geocoding',
            'hybrid_architecture'
        ]
    })

if __name__ == '__main__':
    print("ğŸš€ å¯åŠ¨ Express Parcel Visualization æœåŠ¡å™¨")
    print("ğŸ—ï¸ å®Œæ•´Colabé€»è¾‘é›†æˆï¼šå‰ç«¯CSVè§£æ + åç«¯warehouseä¿®å¤")
    print("ğŸ“ Warehouseä½ç½®è‡ªåŠ¨ä¿®å¤åŠŸèƒ½å·²å¯ç”¨ï¼ˆå®Œæ•´ç‰ˆï¼‰")
    print("ğŸ—ºï¸ ä½¿ç”¨Python KeplerGLåç«¯æ¸²æŸ“")
    print("=" * 70)
    print("ğŸ”§ ç‰¹æ€§ï¼ˆåŸºäºæˆç†ŸColabç‰ˆæœ¬ï¼‰:")
    print("   â€¢ å‰ç«¯CSVæ–‡ä»¶è¯»å–å’Œè§£æï¼ˆæ— ä¸Šä¼ é™åˆ¶ï¼‰")
    print("   â€¢ æ™ºèƒ½warehouse IDåˆ†æå’Œé‚®ç¼–æ¨æµ‹")
    print("   â€¢ è‡ªåŠ¨ä¿®å¤warehouseé‚®ç¼–ï¼ˆåŸºäºIDæ¨¡å¼åŒ¹é…ï¼‰")
    print("   â€¢ è¯¦ç»†çš„æ•°æ®å¤„ç†æ­¥éª¤å’Œæ—¥å¿—")
    print("   â€¢ æ™ºèƒ½åœ°ç†ç¼–ç ï¼ˆAPI + ç¼“å­˜ï¼‰")
    print("   â€¢ å®Œæ•´çš„æ•°æ®æ¸…æ´—å’ŒéªŒè¯æµæ°´çº¿")
    print("   â€¢ äº¤äº’å¼Kepler.glå¯è§†åŒ–ï¼ˆå¤šå›¾å±‚ï¼‰")
    print("   â€¢ æ—¶é—´åºåˆ—è¿‡æ»¤å™¨å’ŒåŠ¨ç”»")
    print("   â€¢ å“åº”å¼Webç•Œé¢ + è°ƒè¯•é¢æ¿")
    print("=" * 70)
    print("ğŸ“‹ æ”¯æŒçš„Warehouseç³»åˆ—:")
    print("   â€¢ NJç³»åˆ—: NJ9, NJ8, NJ7, NJ-Main (æ–°æ³½è¥¿ç‰©æµä¸­å¿ƒ)")
    print("   â€¢ TXç³»åˆ—: TX8828, TX8829, TX-DFW, TX-Houston (å¾·å·æ¢çº½)")
    print("   â€¢ WNTç³»åˆ—: WNT485, WNT486, WNT487 (è¥¿æµ·å²¸ç»ˆç«¯)")
    print("   â€¢ CAç³»åˆ—: CA-LA, CA-SF, CA-OAK (åŠ å·æ¸¯å£)")
    print("   â€¢ IL, GA, FLç³»åˆ—: èŠåŠ å“¥ã€äºšç‰¹å…°å¤§ã€è¿ˆé˜¿å¯†")
    print("=" * 70)
    
    app.run(debug=True, host='0.0.0.0', port=5000)